<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SQL on Paul Cutler</title>
    <link>https://paulcutler.org/tags/sql/</link>
    <description>Recent content in SQL on Paul Cutler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Paul Cutler</copyright>
    <lastBuildDate>Wed, 06 Sep 2017 12:33:39 +0000</lastBuildDate><atom:link href="https://paulcutler.org/tags/sql/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>I Hate SQL (Or how my wife finished NFLPool)</title>
      <link>https://paulcutler.org/blog/2017/09/i-hate-sql-or-how-my-wife-finished-nflpool/</link>
      <pubDate>Wed, 06 Sep 2017 12:33:39 +0000</pubDate>
      
      <guid>https://paulcutler.org/blog/2017/09/i-hate-sql-or-how-my-wife-finished-nflpool/</guid>
      <description>I mentioned in my last post (and a couple others) how invaluable my wife has been in my journey to learn Python. That was turned up to 11 last week and I (we) started working on the scoring calculations.
I started to write the scoring calculations exactly as you would expect a newbie coder to – step by step. With the changes to the data model Kelly recommended, I still have a hard time wrapping my head around how we’ve abstracted the pick information.</description>
    </item>
    
    <item>
      <title>Importing NFL statistics into NFLPool</title>
      <link>https://paulcutler.org/blog/2017/08/importing-nfl-statistics-into-nflpool/</link>
      <pubDate>Wed, 30 Aug 2017 13:03:17 +0000</pubDate>
      
      <guid>https://paulcutler.org/blog/2017/08/importing-nfl-statistics-into-nflpool/</guid>
      <description>NFLPool has been live for almost two weeks – and hasn’t crashed (yet!) After the rush to get the site up and allow a user to make their picks before I left on vacation, there is one more large chunk of work to get to 1.0 release: calculate the score for all players every week of the NFL season.
I spent all of last week in the middle of Minnesota at a friend’s cabin.</description>
    </item>
    
    <item>
      <title>NFLPool Progress: 8/1/2017</title>
      <link>https://paulcutler.org/blog/2017/08/nflpool-progress-812017/</link>
      <pubDate>Wed, 02 Aug 2017 14:53:08 +0000</pubDate>
      
      <guid>https://paulcutler.org/blog/2017/08/nflpool-progress-812017/</guid>
      <description>I took the week off from work to see how much progress I could make on NFLPool this week (and to get some stuff done around the house, but really, for NFLPool). I told myself I’d blog my progress every day and here it is Wednesday already.
In my last blog post, I noted how I was going to have to move back to SQLite for the database. Over the weekend I ripped out most of the MongoDB code and started laying the groundwork for SQLite.</description>
    </item>
    
    <item>
      <title>Back to SQLite for NFLPool</title>
      <link>https://paulcutler.org/blog/2017/07/back-to-sqlite-for-nflpool/</link>
      <pubDate>Mon, 31 Jul 2017 12:47:48 +0000</pubDate>
      
      <guid>https://paulcutler.org/blog/2017/07/back-to-sqlite-for-nflpool/</guid>
      <description>I’ve had to throw in the towel on MongoDB and move back to SQLite for the datastore. For now.
I was successful in being able to call an API, take the JSON object from the API, and store that JSON in MongoDB as a collection. But I what really wanted was to store that JSON object as an EmbeddedDocument within a collection.
My original goal was to stick JSON objects into MongoDB and then query against that.</description>
    </item>
    
    <item>
      <title>Importing Team Data into NFLPool</title>
      <link>https://paulcutler.org/blog/2016/10/importing-team-data-into-nflpool/</link>
      <pubDate>Thu, 13 Oct 2016 14:31:20 +0000</pubDate>
      
      <guid>https://paulcutler.org/blog/2016/10/importing-team-data-into-nflpool/</guid>
      <description>Last weekend I discovered how to pretty print the five JSON files I get from MySportsFeeds. This was helpful to understand just how much data is nested within each file. I also spent a good chunk of the weekend writing in a notebook. I mostly did some data modeling on what each table in the database should store and what their primary keys would be. I also captured things I need to research and started breaking the project into chunks.</description>
    </item>
    
  </channel>
</rss>
